[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Professor Kwangsoo Kim, Department of Statistics, Jeonbuk National University\n2nd semester, 2023\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 16, 2023\n\n\n발표2\n\n\n김보람 \n\n\n\n\nDec 14, 2023\n\n\nVIT_MNIST_Classification\n\n\n김보람 \n\n\n\n\nDec 8, 2023\n\n\n발표\n\n\n김보람 \n\n\n\n\nNov 27, 2023\n\n\n[DL] VAE\n\n\n김보람 \n\n\n\n\nNov 13, 2023\n\n\n[DL] CV_CIFAR(1)\n\n\n김보람 \n\n\n\n\nNov 13, 2023\n\n\n[DL] Att01\n\n\n김보람 \n\n\n\n\nOct 30, 2023\n\n\n[ADL] Take Home\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/발표.html",
    "href": "posts/발표.html",
    "title": "발표",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/발표.html#데이터정리",
    "href": "posts/발표.html#데이터정리",
    "title": "발표",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/발표.html#분석-1gcn",
    "href": "posts/발표.html#분석-1gcn",
    "title": "발표",
    "section": "분석 1(GCN)",
    "text": "분석 1(GCN)\n\ntorch.manual_seed(202250926)\n\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\npred\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.902098\n0.862478\n0.95913\n0.90824\n\n\n\n\n\n\n\n\nb1,W1 = list(model.conv1.parameters())\nb1,W1\n\n(Parameter containing:\n tensor([ 3.3281e-01, -5.2539e-01,  1.1277e+00,  2.8239e-01, -1.2373e-01,\n          9.6607e-01, -1.7652e-02,  8.3960e-01,  3.3252e-01, -4.6080e-01,\n          8.6407e-01,  1.3294e+00,  3.2962e-01, -1.3215e-06,  2.1968e+00,\n         -1.8843e-01,  7.6852e-10, -5.0779e-01, -7.7613e-09, -7.8956e-04,\n         -4.6320e-01, -6.8497e-02,  3.4533e-02, -4.4006e-01, -3.8074e-01,\n         -4.5996e-01,  4.4989e-13, -3.9142e-08,  4.3476e-01,  1.5457e+00,\n         -3.1396e-01,  4.3208e-01], requires_grad=True),\n Parameter containing:\n tensor([[ 1.6974e-01],\n         [ 1.4139e-01],\n         [-6.2153e-03],\n         [ 1.3422e-01],\n         [-8.4340e-02],\n         [-5.0439e-03],\n         [-5.0210e-02],\n         [-4.3200e-03],\n         [ 2.5801e-01],\n         [ 2.1217e-01],\n         [-4.4886e-03],\n         [-7.1453e-03],\n         [ 1.8556e-01],\n         [-3.8936e-02],\n         [-1.2772e-02],\n         [-3.0888e-03],\n         [-1.9216e-05],\n         [ 4.9480e-02],\n         [-6.8162e-02],\n         [-8.9236e-02],\n         [ 1.3753e-01],\n         [-6.8890e-02],\n         [-6.3743e-02],\n         [ 2.1896e-01],\n         [ 2.9288e-03],\n         [ 4.8694e-03],\n         [-4.5865e-02],\n         [-5.2090e-02],\n         [ 1.2880e-01],\n         [-8.3668e-03],\n         [ 9.4231e-03],\n         [ 1.3015e-01]], requires_grad=True))\n\n\n\nb2,W2 = list(model.conv2.parameters())\nb2,W2\n\n(Parameter containing:\n tensor([ 0.8103, -0.8103], requires_grad=True),\n Parameter containing:\n tensor([[-3.1862e-02,  1.2264e-01,  2.5667e-01, -2.9070e-01,  1.9933e-02,\n           2.8287e-01, -8.1168e-03,  1.7061e-01,  3.1362e-02, -1.3049e-01,\n           2.4063e-02,  1.5218e-01,  1.7274e-01,  5.8768e-06,  2.9730e-01,\n           3.7174e-02, -1.1418e-09, -2.0023e-01, -1.3184e-02,  5.8934e-02,\n           2.0550e-01, -5.7726e-02,  2.8572e-01, -1.4259e-01, -2.8006e-01,\n          -6.7342e-02, -3.5160e-02, -7.0944e-02, -5.3993e-02,  1.6846e-01,\n           5.3713e-02, -1.7743e-01],\n         [-2.9673e-02,  1.2650e-01, -1.8649e-01, -2.8863e-01,  2.7340e-01,\n          -2.5555e-02, -2.4640e-02, -9.0935e-02,  3.2923e-02, -1.2820e-01,\n          -2.4358e-01, -2.7795e-01,  1.7514e-01, -4.4449e-06, -3.2877e-01,\n           5.9096e-02,  1.0013e-09, -1.9043e-01,  1.0161e-02, -4.9584e-02,\n           2.0870e-01, -6.1236e-03,  2.2979e-01, -1.4060e-01,  1.1504e-01,\n           2.0197e-01,  4.5961e-02, -1.0499e-01, -5.0285e-02, -3.6542e-01,\n           1.1680e-01, -1.7159e-01]], requires_grad=True))"
  },
  {
    "objectID": "posts/발표.html#분석2로지스틱-회귀",
    "href": "posts/발표.html#분석2로지스틱-회귀",
    "title": "발표",
    "section": "분석2(로지스틱 회귀)",
    "text": "분석2(로지스틱 회귀)\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\nlrnr.fit(X,y)\n\n#thresh = y.mean()\n#yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\nyyhat\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.849484\n0.933279\n0.756098\n0.835397"
  },
  {
    "objectID": "posts/발표.html#분석3xgboost",
    "href": "posts/발표.html#분석3xgboost",
    "title": "발표",
    "section": "분석3(XGBoost)",
    "text": "분석3(XGBoost)\n\nimport xgboost as xgb\n\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nlrnr = xgb.XGBClassifier()\n \nlrnr.fit(X,y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.88012\n0.886957\n0.874094\n0.880478"
  },
  {
    "objectID": "posts/발표.html#분석4light-gbm",
    "href": "posts/발표.html#분석4light-gbm",
    "title": "발표",
    "section": "분석4(Light GBM)",
    "text": "분석4(Light GBM)\n\nimport lightgbm as lgb\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = lgb.LGBMClassifier()\n\nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results4 = pd.DataFrame({m.__name__: [m(yy, yyhat).round(6)] for m in metrics}, index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.885115\n0.893817\n0.87673\n0.885191"
  },
  {
    "objectID": "posts/발표.html#분석5one-class-svm",
    "href": "posts/발표.html#분석5one-class-svm",
    "title": "발표",
    "section": "분석5(One class SVM)",
    "text": "분석5(One class SVM)\nfrom sklearn.svm import OneClassSVM\n\ntorch.manual_seed(202250926)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nocsvm = OneClassSVM(gamma='auto')  \nocsvm.fit(X[y == 0])  \n\n\nyyhat_ocsvm = ocsvm.predict(XX)\n\nyyhat_ocsvm_binary = np.where(yyhat_ocsvm ==  1, 0)\n\n\nmetrics_ocsvm = [\n    sklearn.metrics.accuracy_score,\n    sklearn.metrics.precision_score,\n    sklearn.metrics.recall_score,\n    sklearn.metrics.f1_score\n]\n\n_results5 = pd.DataFrame({m.__name__: [m(yy, yyhat_ocsvm_binary).round(6)] for m in metrics_ocsvm}, index=['분석5'])\n_results5"
  },
  {
    "objectID": "posts/CV_CIFAR(1).html",
    "href": "posts/CV_CIFAR(1).html",
    "title": "[DL] CV_CIFAR(1)",
    "section": "",
    "text": "!pip install torcheval\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torcheval.metrics.functional import multiclass_f1_score\nfrom torchvision import datasets, transforms, models\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom torchsummary import summary\n\nis_cuda = torch.cuda.is_available()\nprint(is_cuda)\ndevice = torch.device('cuda' if is_cuda else 'cpu')\nprint('Current cuda device is', device)\n\nRequirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\nTrue\nCurrent cuda device is cuda\n\n\n\n!nvcc --version\nprint(\"Torch version:{}\".format(torch.__version__))\nprint(\"cuda version: {}\".format(torch.version.cuda))\nprint(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nTorch version:2.1.0+cu118\ncuda version: 11.8\ncudnn version:8700\n\n\n\ntrain_data = datasets.CIFAR10(root = './data/02/',\n                            train=True,\n                            download=True,\n                            transform=transforms.ToTensor())\n\ntest_data = datasets.CIFAR10(root = './data/02/',\n                            train=False,\n                            download=True,\n                            transform=transforms.ToTensor())\nprint('number of training data : ', len(train_data))\nprint('number of test data : ', len(test_data))\n\n# transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n\n# Download and load the training data\ntrain_data = datasets.CIFAR10('./data', download=True, train=True,\n                              transform=transforms.Compose([transforms.Resize(224),transforms.ToTensor()])\n)\n# Download and load the test data\ntest_data = datasets.CIFAR10('./data', download=True, train=False,\n                              transform=transforms.Compose([transforms.Resize(224),transforms.ToTensor()])\n)\n\nprint('number of training data : ', len(train_data))\nprint('number of test data : ', len(test_data))\n\nFiles already downloaded and verified\nFiles already downloaded and verified\nnumber of training data :  50000\nnumber of test data :  10000\nFiles already downloaded and verified\nFiles already downloaded and verified\nnumber of training data :  50000\nnumber of test data :  10000\n\n\n\n원래는 32x32x3인데 224x224로 transforms 함수 써서 바꿈\n\n\nimage, label = train_data[10000]\nimage = image.permute(1, 2, 0)\nprint(image.shape)\n\nplt.figure()\nplt.imshow(image.numpy())\nplt.title('label : %s' % label)\nplt.show()\n\n#plt.imshow(image).squeeze().numpy(), cmap='rgb')\n#plt.show()\n\ntorch.Size([224, 224, 3])\n\n\n\n\n\n\nbatch_size = 100\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data,\n                                           batch_size = batch_size, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_data,\n                                           batch_size = batch_size, shuffle = True)\ntest_loaderA = torch.utils.data.DataLoader(dataset=test_data,\n                                           batch_size = batch_size, shuffle = True)\nfirst_batch = train_loader.__iter__().__next__()\n\n\nprint('{:15s} | {:&lt;25s} | {}'.format('name', 'type', 'size'))\nprint('{:15s} | {:&lt;25s} | {}'.format('Num of Batch', '', len(train_loader)))\nprint('{:15s} | {:&lt;25s} | {}'.format('first_batch', str(type(first_batch)), len(first_batch)))\nprint('{:15s} | {:&lt;25s} | {}'.format('first_batch[0]', str(type(first_batch[0])), first_batch[0].shape))\nprint('{:15s} | {:&lt;25s} | {}'.format('first_batch[1]', str(type(first_batch[1])), first_batch[1].shape))\n\nname            | type                      | size\nNum of Batch    |                           | 500\nfirst_batch     | &lt;class 'list'&gt;            | 2\nfirst_batch[0]  | &lt;class 'torch.Tensor'&gt;    | torch.Size([100, 3, 224, 224])\nfirst_batch[1]  | &lt;class 'torch.Tensor'&gt;    | torch.Size([100])\n\n\n\nresnet18_pretrained = models.resnet18(pretrained=True)\nresnet18_pretrained.to(\"cuda\")\n# summary(resnet18_pretrained, input_size=(3, 224, 224))\nfor param in resnet18_pretrained.parameters():\n    param.requires_grad = True  # Weights Freeze\n\nfor param in resnet18_pretrained.parameters():\n    param.requires_grad = True\n\nnum_ftrs = resnet18_pretrained.fc.in_features\nprint(num_ftrs) # ResNet18모델의 마지막 단에서, 출력 노드의 갯수를 구해주는 함수\nresnet18_pretrained.fc = nn.Linear(num_ftrs, 10)\nresnet18_pretrained.sf = nn.Softmax(dim=1)\nresnet18_pretrained = resnet18_pretrained.to(device)\n\n#print(resnet18_pretrained)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n512\n\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=(1,1))   #224*224   # (in-channe, out-channel,  , strinding)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1, padding=(1,1))  #same\n        self.conv3 = nn.Conv2d(32, 100, 3, 1, padding=(1,1)) #same\n        self.dropout = nn.Dropout2d(0.25)\n        # (입력 뉴런, 출력 뉴런)\n        self.fc1 = nn.Linear(313600, 1000)    # 56*56*100 = 313600\n        self.fc2 = nn.Linear(1000, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)  # 반절 줄어\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)  # 반절 줄어\n        x = self.conv3(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\nmodel = CNN().to(device)\n#print(model)\n\nlearning_rate = 0.001\noptimizer = optim.Adam(resnet18_pretrained.parameters(), lr = learning_rate)\noptimizer0 = optim.Adam(model.parameters(), lr = learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n위에를 통틀어서 convolution이라고 하기도 함\n\nresnet18_pretrained.train()\n#model.train()\n\nepoch_num = 2\ni = 1\nfor epoch in range(epoch_num):\n    for data, target in train_loader:\n        data = data.to(device)\n        target = target.to(device)\n        optimizer.zero_grad()\n        output = resnet18_pretrained(data)\n        #output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if i % 100 == 0:\n            print(\"Train Step :{}  {}\\tLoss : {:3f}\".format(epoch, i, loss.item()))\n        i += 1\n\nTrain Step :0  100  Loss : 0.631524\nTrain Step :0  200  Loss : 0.641870\nTrain Step :0  300  Loss : 0.540941\nTrain Step :0  400  Loss : 0.372230\nTrain Step :0  500  Loss : 0.451633\nTrain Step :1  600  Loss : 0.276236\nTrain Step :1  700  Loss : 0.277767\nTrain Step :1  800  Loss : 0.369319\nTrain Step :1  900  Loss : 0.365539\nTrain Step :1  1000 Loss : 0.372553\n\n\n\nloss값이 strictly하게 감소는 안하지만.. 경양??적으로 감소함 (왔다 갔다 하면서 감소)\n\n\n\nresnet18_pretrained.eval()\ncorrect = 0\n\nfor data, target in test_loader:\n    data = data.to(device)\n    target = target.to(device)\n    output = resnet18_pretrained(data)\n    prediction = output.data.max(1)[1]\n    correct += prediction.eq(target.data).sum()\n\nprint('Test set Accuracy : {:.2f}%'.format(100. * correct / len(test_loader.dataset)))\n\nf1 = 0\nip = 0\nresnet18_pretrained.eval()\nfor data, target in test_loader:\n    data = data.to(device)\n    target = target.to(device)\n    output = resnet18_pretrained(data)\n    prediction = output.data.max(1)[1]\n    ip += 1\n    f1 += multiclass_f1_score(prediction, target, num_classes=10, average='micro')\n\nprint(f1/ip)\n\ntensor(0.8919, device='cuda:0')"
  },
  {
    "objectID": "posts/VIT_MNIST_Classification__(1).html",
    "href": "posts/VIT_MNIST_Classification__(1).html",
    "title": "VIT_MNIST_Classification",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split, DataLoader\n\nfrom torchvision import datasets, transforms, models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom torchvision.utils import save_image\n\nfrom torchsummary import summary\n\nimport spacy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nimport time\nimport math\nfrom PIL import Image\nimport glob\nfrom IPython.display import display\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ncuda\n\n\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n\nBATCH_SIZE = 200\nLR = 5e-5\nNUM_EPOCHES = 10\n\n\nmean, std = (0.5,), (0.5,)\n\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize(mean, std)\n                              ])\n\n\ntrainset = datasets.MNIST('../data/MNIST/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n\ntestset = datasets.MNIST('../data/MNIST/', download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw\n\n\n\n100%|██████████| 9912422/9912422 [00:00&lt;00:00, 132524476.35it/s]\n100%|██████████| 28881/28881 [00:00&lt;00:00, 40378564.61it/s]\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 38440411.96it/s]\n100%|██████████| 4542/4542 [00:00&lt;00:00, 22842360.63it/s]\n\n\n\n\n\n!pip install transformer-implementations\nfrom transformer_package.models import ViT\n\nCollecting transformer-implementations\n  Downloading transformer_implementations-0.0.9-py3-none-any.whl (9.4 kB)\nInstalling collected packages: transformer-implementations\nSuccessfully installed transformer-implementations-0.0.9\n\n\n\nimage_size = 28\nchannel_size = 1\npatch_size = 7\nembed_size = 512\nnum_heads = 4\nclasses = 10\nnum_layers = 2\nhidden_size = 256\ndropout = 0.2\n\nmodel = ViT(image_size, channel_size, patch_size, embed_size, num_heads, classes, num_layers, hidden_size, dropout=dropout).to(device)\nmodel\n\nViT(\n  (dropout_layer): Dropout(p=0.2, inplace=False)\n  (embeddings): Linear(in_features=49, out_features=512, bias=True)\n  (encoders): ModuleList(\n    (0-1): 2 x VisionEncoder(\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (attention): MultiHeadAttention(\n        (dropout_layer): Dropout(p=0.2, inplace=False)\n        (Q): Linear(in_features=512, out_features=512, bias=True)\n        (K): Linear(in_features=512, out_features=512, bias=True)\n        (V): Linear(in_features=512, out_features=512, bias=True)\n        (linear): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): GELU(approximate='none')\n        (2): Dropout(p=0.2, inplace=False)\n        (3): Linear(in_features=2048, out_features=512, bias=True)\n        (4): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (classifier): Sequential(\n    (0): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\n\nfor img, label in trainloader:\n    img = img.to(device)\n    label = label.to(device)\n\n    print(\"Input Image Dimensions: {}\".format(img.size()))\n    print(\"Label Dimensions: {}\".format(label.size()))\n    print(\"-\"*100)\n\n    out = model(img)\n\n    print(\"Output Dimensions: {}\".format(out.size()))\n    break\n\nInput Image Dimensions: torch.Size([200, 1, 28, 28])\nLabel Dimensions: torch.Size([200])\n----------------------------------------------------------------------------------------------------\nOutput Dimensions: torch.Size([200, 10])\n\n\n\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=LR)\n\n\nloss_hist = {}\nloss_hist[\"train accuracy\"] = []\nloss_hist[\"train loss\"] = []\n\nfor epoch in range(1, NUM_EPOCHES+1):\n    model.train()\n\n    epoch_train_loss = 0\n\n    y_true_train = []\n    y_pred_train = []\n    ip = 0\n\n    for batch_idx, (img, labels) in enumerate(trainloader):\n        img = img.to(device)\n        labels = labels.to(device)\n\n        preds = model(img)\n\n        loss = criterion(preds, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        y_pred_train.extend(preds.detach().argmax(dim=-1).tolist())\n        y_true_train.extend(labels.detach().tolist())\n\n        epoch_train_loss += loss.item()\n        ip = ip + 1\n        if ip % 100 == 0:\n            print(\"Step: {:.8f}  {:.8f}\".format(epoch, ip))\n\n    loss_hist[\"train loss\"].append(epoch_train_loss)\n\n    total_correct = len([True for x, y in zip(y_pred_train, y_true_train) if x==y])\n    total = len(y_pred_train)\n    accuracy = total_correct * 100 / total\n\n    loss_hist[\"train accuracy\"].append(accuracy)\n\n    print(\"-------------------------------------------------\")\n    print(\"Epoch: {} Train mean loss: {:.8f}\".format(epoch, epoch_train_loss))\n    print(\"       Train Accuracy%: \", accuracy, \"==\", total_correct, \"/\", total)\n    print(\"-------------------------------------------------\")\n\nStep: 1.00000000  100.00000000\nStep: 1.00000000  200.00000000\nStep: 1.00000000  300.00000000\n-------------------------------------------------\nEpoch: 1 Train mean loss: 323.08200696\n       Train Accuracy%:  63.598333333333336 == 38159 / 60000\n-------------------------------------------------\nStep: 2.00000000  100.00000000\nStep: 2.00000000  200.00000000\nStep: 2.00000000  300.00000000\n-------------------------------------------------\nEpoch: 2 Train mean loss: 131.86424088\n       Train Accuracy%:  86.045 == 51627 / 60000\n-------------------------------------------------\nStep: 3.00000000  100.00000000\nStep: 3.00000000  200.00000000\nStep: 3.00000000  300.00000000\n-------------------------------------------------\nEpoch: 3 Train mean loss: 102.83779885\n       Train Accuracy%:  89.11833333333334 == 53471 / 60000\n-------------------------------------------------\nStep: 4.00000000  100.00000000\nStep: 4.00000000  200.00000000\nStep: 4.00000000  300.00000000\n-------------------------------------------------\nEpoch: 4 Train mean loss: 87.88007259\n       Train Accuracy%:  90.72666666666667 == 54436 / 60000\n-------------------------------------------------\nStep: 5.00000000  100.00000000\nStep: 5.00000000  200.00000000\nStep: 5.00000000  300.00000000\n-------------------------------------------------\nEpoch: 5 Train mean loss: 78.15140389\n       Train Accuracy%:  91.76166666666667 == 55057 / 60000\n-------------------------------------------------\nStep: 6.00000000  100.00000000\nStep: 6.00000000  200.00000000\nStep: 6.00000000  300.00000000\n-------------------------------------------------\nEpoch: 6 Train mean loss: 69.25521898\n       Train Accuracy%:  92.69333333333333 == 55616 / 60000\n-------------------------------------------------\nStep: 7.00000000  100.00000000\nStep: 7.00000000  200.00000000\nStep: 7.00000000  300.00000000\n-------------------------------------------------\nEpoch: 7 Train mean loss: 63.42148008\n       Train Accuracy%:  93.325 == 55995 / 60000\n-------------------------------------------------\nStep: 8.00000000  100.00000000\nStep: 8.00000000  200.00000000\nStep: 8.00000000  300.00000000\n-------------------------------------------------\nEpoch: 8 Train mean loss: 58.87239636\n       Train Accuracy%:  93.83333333333333 == 56300 / 60000\n-------------------------------------------------\nStep: 9.00000000  100.00000000\nStep: 9.00000000  200.00000000\nStep: 9.00000000  300.00000000\n-------------------------------------------------\nEpoch: 9 Train mean loss: 54.04052846\n       Train Accuracy%:  94.25166666666667 == 56551 / 60000\n-------------------------------------------------\nStep: 10.00000000  100.00000000\nStep: 10.00000000  200.00000000\nStep: 10.00000000  300.00000000\n-------------------------------------------------\nEpoch: 10 Train mean loss: 50.72075617\n       Train Accuracy%:  94.59333333333333 == 56756 / 60000\n-------------------------------------------------\n\n\n\n\n\n\nplt.plot(loss_hist[\"train accuracy\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n\n\n\n\nplt.plot(loss_hist[\"train loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n\n\n\n\nwith torch.no_grad():\n    model.eval()\n\n    y_true_test = []\n    y_pred_test = []\n\n    for batch_idx, (img, labels) in enumerate(testloader):\n        img = img.to(device)\n        label = label.to(device)\n\n        preds = model(img)\n\n        y_pred_test.extend(preds.detach().argmax(dim=-1).tolist())\n        y_true_test.extend(labels.detach().tolist())\n\n    total_correct = len([True for x, y in zip(y_pred_test, y_true_test) if x==y])\n    total = len(y_pred_test)\n    accuracy = total_correct * 100 / total\n\n    print(\"Test Accuracy%: \", accuracy, \"==\", total_correct, \"/\", total)\n\nTest Accuracy%:  96.43 == 9643 / 10000"
  },
  {
    "objectID": "posts/VIT_MNIST_Classification__(1).html#model",
    "href": "posts/VIT_MNIST_Classification__(1).html#model",
    "title": "VIT_MNIST_Classification",
    "section": "",
    "text": "!pip install transformer-implementations\nfrom transformer_package.models import ViT\n\nCollecting transformer-implementations\n  Downloading transformer_implementations-0.0.9-py3-none-any.whl (9.4 kB)\nInstalling collected packages: transformer-implementations\nSuccessfully installed transformer-implementations-0.0.9\n\n\n\nimage_size = 28\nchannel_size = 1\npatch_size = 7\nembed_size = 512\nnum_heads = 4\nclasses = 10\nnum_layers = 2\nhidden_size = 256\ndropout = 0.2\n\nmodel = ViT(image_size, channel_size, patch_size, embed_size, num_heads, classes, num_layers, hidden_size, dropout=dropout).to(device)\nmodel\n\nViT(\n  (dropout_layer): Dropout(p=0.2, inplace=False)\n  (embeddings): Linear(in_features=49, out_features=512, bias=True)\n  (encoders): ModuleList(\n    (0-1): 2 x VisionEncoder(\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (attention): MultiHeadAttention(\n        (dropout_layer): Dropout(p=0.2, inplace=False)\n        (Q): Linear(in_features=512, out_features=512, bias=True)\n        (K): Linear(in_features=512, out_features=512, bias=True)\n        (V): Linear(in_features=512, out_features=512, bias=True)\n        (linear): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): GELU(approximate='none')\n        (2): Dropout(p=0.2, inplace=False)\n        (3): Linear(in_features=2048, out_features=512, bias=True)\n        (4): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (classifier): Sequential(\n    (0): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\n\nfor img, label in trainloader:\n    img = img.to(device)\n    label = label.to(device)\n\n    print(\"Input Image Dimensions: {}\".format(img.size()))\n    print(\"Label Dimensions: {}\".format(label.size()))\n    print(\"-\"*100)\n\n    out = model(img)\n\n    print(\"Output Dimensions: {}\".format(out.size()))\n    break\n\nInput Image Dimensions: torch.Size([200, 1, 28, 28])\nLabel Dimensions: torch.Size([200])\n----------------------------------------------------------------------------------------------------\nOutput Dimensions: torch.Size([200, 10])\n\n\n\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=LR)\n\n\nloss_hist = {}\nloss_hist[\"train accuracy\"] = []\nloss_hist[\"train loss\"] = []\n\nfor epoch in range(1, NUM_EPOCHES+1):\n    model.train()\n\n    epoch_train_loss = 0\n\n    y_true_train = []\n    y_pred_train = []\n    ip = 0\n\n    for batch_idx, (img, labels) in enumerate(trainloader):\n        img = img.to(device)\n        labels = labels.to(device)\n\n        preds = model(img)\n\n        loss = criterion(preds, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        y_pred_train.extend(preds.detach().argmax(dim=-1).tolist())\n        y_true_train.extend(labels.detach().tolist())\n\n        epoch_train_loss += loss.item()\n        ip = ip + 1\n        if ip % 100 == 0:\n            print(\"Step: {:.8f}  {:.8f}\".format(epoch, ip))\n\n    loss_hist[\"train loss\"].append(epoch_train_loss)\n\n    total_correct = len([True for x, y in zip(y_pred_train, y_true_train) if x==y])\n    total = len(y_pred_train)\n    accuracy = total_correct * 100 / total\n\n    loss_hist[\"train accuracy\"].append(accuracy)\n\n    print(\"-------------------------------------------------\")\n    print(\"Epoch: {} Train mean loss: {:.8f}\".format(epoch, epoch_train_loss))\n    print(\"       Train Accuracy%: \", accuracy, \"==\", total_correct, \"/\", total)\n    print(\"-------------------------------------------------\")\n\nStep: 1.00000000  100.00000000\nStep: 1.00000000  200.00000000\nStep: 1.00000000  300.00000000\n-------------------------------------------------\nEpoch: 1 Train mean loss: 323.08200696\n       Train Accuracy%:  63.598333333333336 == 38159 / 60000\n-------------------------------------------------\nStep: 2.00000000  100.00000000\nStep: 2.00000000  200.00000000\nStep: 2.00000000  300.00000000\n-------------------------------------------------\nEpoch: 2 Train mean loss: 131.86424088\n       Train Accuracy%:  86.045 == 51627 / 60000\n-------------------------------------------------\nStep: 3.00000000  100.00000000\nStep: 3.00000000  200.00000000\nStep: 3.00000000  300.00000000\n-------------------------------------------------\nEpoch: 3 Train mean loss: 102.83779885\n       Train Accuracy%:  89.11833333333334 == 53471 / 60000\n-------------------------------------------------\nStep: 4.00000000  100.00000000\nStep: 4.00000000  200.00000000\nStep: 4.00000000  300.00000000\n-------------------------------------------------\nEpoch: 4 Train mean loss: 87.88007259\n       Train Accuracy%:  90.72666666666667 == 54436 / 60000\n-------------------------------------------------\nStep: 5.00000000  100.00000000\nStep: 5.00000000  200.00000000\nStep: 5.00000000  300.00000000\n-------------------------------------------------\nEpoch: 5 Train mean loss: 78.15140389\n       Train Accuracy%:  91.76166666666667 == 55057 / 60000\n-------------------------------------------------\nStep: 6.00000000  100.00000000\nStep: 6.00000000  200.00000000\nStep: 6.00000000  300.00000000\n-------------------------------------------------\nEpoch: 6 Train mean loss: 69.25521898\n       Train Accuracy%:  92.69333333333333 == 55616 / 60000\n-------------------------------------------------\nStep: 7.00000000  100.00000000\nStep: 7.00000000  200.00000000\nStep: 7.00000000  300.00000000\n-------------------------------------------------\nEpoch: 7 Train mean loss: 63.42148008\n       Train Accuracy%:  93.325 == 55995 / 60000\n-------------------------------------------------\nStep: 8.00000000  100.00000000\nStep: 8.00000000  200.00000000\nStep: 8.00000000  300.00000000\n-------------------------------------------------\nEpoch: 8 Train mean loss: 58.87239636\n       Train Accuracy%:  93.83333333333333 == 56300 / 60000\n-------------------------------------------------\nStep: 9.00000000  100.00000000\nStep: 9.00000000  200.00000000\nStep: 9.00000000  300.00000000\n-------------------------------------------------\nEpoch: 9 Train mean loss: 54.04052846\n       Train Accuracy%:  94.25166666666667 == 56551 / 60000\n-------------------------------------------------\nStep: 10.00000000  100.00000000\nStep: 10.00000000  200.00000000\nStep: 10.00000000  300.00000000\n-------------------------------------------------\nEpoch: 10 Train mean loss: 50.72075617\n       Train Accuracy%:  94.59333333333333 == 56756 / 60000\n-------------------------------------------------"
  },
  {
    "objectID": "posts/VIT_MNIST_Classification__(1).html#test",
    "href": "posts/VIT_MNIST_Classification__(1).html#test",
    "title": "VIT_MNIST_Classification",
    "section": "",
    "text": "plt.plot(loss_hist[\"train accuracy\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n\n\n\n\nplt.plot(loss_hist[\"train loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n\n\n\n\nwith torch.no_grad():\n    model.eval()\n\n    y_true_test = []\n    y_pred_test = []\n\n    for batch_idx, (img, labels) in enumerate(testloader):\n        img = img.to(device)\n        label = label.to(device)\n\n        preds = model(img)\n\n        y_pred_test.extend(preds.detach().argmax(dim=-1).tolist())\n        y_true_test.extend(labels.detach().tolist())\n\n    total_correct = len([True for x, y in zip(y_pred_test, y_true_test) if x==y])\n    total = len(y_pred_test)\n    accuracy = total_correct * 100 / total\n\n    print(\"Test Accuracy%: \", accuracy, \"==\", total_correct, \"/\", total)\n\nTest Accuracy%:  96.43 == 9643 / 10000"
  },
  {
    "objectID": "posts/VAE.html",
    "href": "posts/VAE.html",
    "title": "[DL] VAE",
    "section": "",
    "text": "!pip install torcheval\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom torchvision.utils import save_image, make_grid\n\nis_cuda = torch.cuda.is_available()\nprint(is_cuda)\ndevice = torch.device('cuda' if is_cuda else 'cpu')\nprint('Current cuda device is', device)\n\nCollecting torcheval\n  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.7 MB/s eta 0:00:00 0:00:01\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\nInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\nTrue\nCurrent cuda device is cuda\n\n\n\n!nvcc --version\nprint(\"Torch version:{}\".format(torch.__version__))\nprint(\"cuda version: {}\".format(torch.version.cuda))\nprint(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nTorch version:2.1.0+cu118\ncuda version: 11.8\ncudnn version:8700\n\n\n\ntransform = transforms.Compose([transforms.ToTensor()])\n\n# download the MNIST datasets\npath = '~/datasets'\ntrain_dataset = MNIST(path, transform=transform, download=True)\ntest_dataset  = MNIST(path, transform=transform, download=True)\n\n# create train and test dataloaders\nbatch_size = 100\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/datasets/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting /root/datasets/MNIST/raw/train-images-idx3-ubyte.gz to /root/datasets/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting /root/datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting /root/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/datasets/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting /root/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw\n\n\n\n100%|██████████| 9912422/9912422 [00:00&lt;00:00, 136550644.38it/s]\n100%|██████████| 28881/28881 [00:00&lt;00:00, 119936330.52it/s]\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 204267696.39it/s]\n100%|██████████| 4542/4542 [00:00&lt;00:00, 2259849.20it/s]\n\n\n\nimage = next(iter(train_loader))\n\nnum_samples = 25\nsample_images = [image[0][i+1,0] for i in range(num_samples)]\n\nfig = plt.figure(figsize=(5, 5))\ngrid = ImageGrid(fig, 111, nrows_ncols=(5, 5), axes_pad=0.1)\n\nfor ax, im in zip(grid, sample_images):\n    ax.imshow(im, cmap='gray')\n    ax.axis('off')\n\nplt.show()\n\n\n\n\n\nclass VAE(nn.Module):\n\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=200, device=device):\n        super(VAE, self).__init__()\n\n        # encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, latent_dim),\n            nn.LeakyReLU(0.2)\n            )\n\n        # latent mean and variance\n        self.mean_layer = nn.Linear(latent_dim, 2)\n        self.logvar_layer = nn.Linear(latent_dim, 2)\n\n        # decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(2, latent_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(latent_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n            )\n\n    def encode(self, x):\n        x = self.encoder(x)\n        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n        return mean, logvar\n\n    def reparameterization(self, mean, var):\n        epsilon = torch.randn_like(var).to(device)\n        z = mean + var*epsilon\n        return z\n\n    def decode(self, x):\n        return self.decoder(x)\n\n    def forward(self, x):\n        mean, log_var = self.encode(x)\n        z = self.reparameterization(mean, log_var)\n        x_hat = self.decode(z)\n        return x_hat, mean, log_var\n\n\nmodel = VAE().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef loss_function(x, x_hat, mean, log_var):\n    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n\n    return reproduction_loss + KLD\n\n\nx_dim = 784\n\ndef train(model, optimizer, epochs, device):\n    model.train()\n    for epoch in range(epochs):\n        overall_loss = 0\n        for batch_idx, (x, _) in enumerate(train_loader):\n            x = x.view(batch_size, x_dim).to(device)\n\n            optimizer.zero_grad()\n\n            x_hat, mean, log_var = model(x)\n            loss = loss_function(x, x_hat, mean, log_var)\n\n            overall_loss += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n    return overall_loss\n\ntrain(model, optimizer, epochs=50, device=device)\n\n    Epoch 1     Average Loss:  175.29515954324916\n    Epoch 2     Average Loss:  157.1823385981845\n    Epoch 3     Average Loss:  152.55317100766902\n    Epoch 4     Average Loss:  149.62154792492697\n    Epoch 5     Average Loss:  147.50248990831074\n    Epoch 6     Average Loss:  145.86534660632304\n    Epoch 7     Average Loss:  144.4231314560726\n    Epoch 8     Average Loss:  143.31780508203778\n    Epoch 9     Average Loss:  142.30168281771702\n    Epoch 10    Average Loss:  141.49097752438962\n    Epoch 11    Average Loss:  140.83598308378546\n    Epoch 12    Average Loss:  140.17502820455968\n    Epoch 13    Average Loss:  139.8000655063126\n    Epoch 14    Average Loss:  139.30528065982367\n    Epoch 15    Average Loss:  138.87654218619573\n    Epoch 16    Average Loss:  138.48964087280885\n    Epoch 17    Average Loss:  138.1048076383817\n    Epoch 18    Average Loss:  137.74657612948664\n    Epoch 19    Average Loss:  137.45947488979027\n    Epoch 20    Average Loss:  137.23071929778797\n    Epoch 21    Average Loss:  136.92694877204195\n    Epoch 22    Average Loss:  136.66642131416944\n    Epoch 23    Average Loss:  136.39156210872287\n    Epoch 24    Average Loss:  136.2546355905676\n    Epoch 25    Average Loss:  136.01997696355906\n    Epoch 26    Average Loss:  135.8481364611592\n    Epoch 27    Average Loss:  135.6641933985027\n    Epoch 28    Average Loss:  135.42658317247495\n    Epoch 29    Average Loss:  135.3036438660789\n    Epoch 30    Average Loss:  135.2169284745409\n    Epoch 31    Average Loss:  134.92463263968594\n    Epoch 32    Average Loss:  134.92840655650042\n    Epoch 33    Average Loss:  134.6508390122861\n    Epoch 34    Average Loss:  134.68385046040277\n    Epoch 35    Average Loss:  134.47515039714628\n    Epoch 36    Average Loss:  134.2281339178579\n    Epoch 37    Average Loss:  134.29903941464943\n    Epoch 38    Average Loss:  134.0389246074186\n    Epoch 39    Average Loss:  133.95940814443344\n    Epoch 40    Average Loss:  133.74494802535474\n    Epoch 41    Average Loss:  133.6946707076899\n    Epoch 42    Average Loss:  133.62954862922578\n    Epoch 43    Average Loss:  133.38309982783807\n    Epoch 44    Average Loss:  133.46671858696786\n    Epoch 45    Average Loss:  133.28396186026188\n    Epoch 46    Average Loss:  133.149587479784\n    Epoch 47    Average Loss:  133.28342907123852\n    Epoch 48    Average Loss:  132.93941725792988\n    Epoch 49    Average Loss:  132.92503653550187\n    Epoch 50    Average Loss:  132.90003576925085\n\n\n7960712.142578125\n\n\n\ndef generate_digit(mean, var):\n    z_sample = torch.tensor([[mean, var]], dtype=torch.float).to(device)\n    print(z_sample)\n    x_decoded = model.decode(z_sample)\n    digit = x_decoded.detach().cpu().reshape(28, 28)  # reshape vector to 2d array\n    plt.imshow(digit, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ngenerate_digit(0.7,-1.0)\n\ntensor([[ 0.7000, -1.0000]], device='cuda:0')\n\n\n\n\n\n\ndef plot_latent_space(model, scale=1.0, n=25, digit_size=28, figsize=15):\n    # display a n*n 2D manifold of digits\n    figure = np.zeros((digit_size * n, digit_size * n))\n\n    # construct a grid\n    grid_x = np.linspace(-scale, scale, n)\n    grid_y = np.linspace(-scale, scale, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = torch.tensor([[xi, yi]], dtype=torch.float).to(device)\n            x_decoded = model.decode(z_sample)\n            digit = x_decoded[0].detach().cpu().reshape(digit_size, digit_size)\n            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size,] = digit\n\n    plt.figure(figsize=(figsize, figsize))\n    plt.title('VAE Latent Space Visualization')\n    start_range = digit_size // 2\n    end_range = n * digit_size + start_range\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"mean, z [0]\")\n    plt.ylabel(\"var, z [1]\")\n    plt.imshow(figure, cmap=\"Greys_r\")\n    plt.show()\n\nplot_latent_space(model)"
  },
  {
    "objectID": "posts/발표v2.html",
    "href": "posts/발표v2.html",
    "title": "발표2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n\n\n# 불균형 데이터 처리\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\n\n# 고객별 거래 시간 차이 \ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n# GCN_model_train/test \ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\n\n# wieght 값 설정-&gt; edge_index 적용\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected"
  },
  {
    "objectID": "posts/발표v2.html#데이터정리",
    "href": "posts/발표v2.html#데이터정리",
    "title": "발표2",
    "section": "데이터정리",
    "text": "데이터정리\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\ntr/test\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\n\n\nedge_index 설정\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\ndata설정(x, edge_index, y)\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/발표v2.html#분석-1gcn",
    "href": "posts/발표v2.html#분석-1gcn",
    "title": "발표2",
    "section": "분석 1(GCN)",
    "text": "분석 1(GCN)\n\ntorch.manual_seed(202250926)\n\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.902098\n0.862478\n0.95913\n0.90824"
  },
  {
    "objectID": "posts/발표v2.html#분석2로지스틱-회귀",
    "href": "posts/발표v2.html#분석2로지스틱-회귀",
    "title": "발표2",
    "section": "분석2(로지스틱 회귀)",
    "text": "분석2(로지스틱 회귀)\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\nlrnr.fit(X,y)\n\n#thresh = y.mean()\n#yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\nyyhat\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.849484\n0.933279\n0.756098\n0.835397"
  },
  {
    "objectID": "posts/발표v2.html#분석3xgboost",
    "href": "posts/발표v2.html#분석3xgboost",
    "title": "발표2",
    "section": "분석3(XGBoost)",
    "text": "분석3(XGBoost)\n\nimport xgboost as xgb\n\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nlrnr = xgb.XGBClassifier()\n \nlrnr.fit(X,y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.88012\n0.886957\n0.874094\n0.880478"
  },
  {
    "objectID": "posts/발표v2.html#분석4light-gbm",
    "href": "posts/발표v2.html#분석4light-gbm",
    "title": "발표2",
    "section": "분석4(Light GBM)",
    "text": "분석4(Light GBM)\n\nimport lightgbm as lgb\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = lgb.LGBMClassifier()\n\nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results4 = pd.DataFrame({m.__name__: [m(yy, yyhat).round(6)] for m in metrics}, index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.885115\n0.893817\n0.87673\n0.885191"
  },
  {
    "objectID": "posts/발표v2.html#분석5one-class-svm",
    "href": "posts/발표v2.html#분석5one-class-svm",
    "title": "발표2",
    "section": "분석5(One class SVM)",
    "text": "분석5(One class SVM)\n\nfrom sklearn.svm import OneClassSVM\n\ntorch.manual_seed(202250926)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nocsvm = OneClassSVM(gamma='auto')  \nocsvm.fit(X[y == 0])  \n\n\nyyhat_ocsvm = ocsvm.predict(XX)\n\nyyhat_ocsvm_binary = np.where(yyhat_ocsvm == 1, 0, 1)\n\n\n\nmetrics_ocsvm = [\n    sklearn.metrics.accuracy_score,\n    sklearn.metrics.precision_score,\n    sklearn.metrics.recall_score,\n    sklearn.metrics.f1_score\n]\n\n_results5 = pd.DataFrame({m.__name__: [m(yy, yyhat_ocsvm_binary).round(6)] for m in metrics_ocsvm}, index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.658342\n0.614559\n0.868161\n0.719672"
  },
  {
    "objectID": "posts/발표v2.html#끄적끄적",
    "href": "posts/발표v2.html#끄적끄적",
    "title": "발표2",
    "section": "끄적끄적",
    "text": "끄적끄적\n-\n\\(A\\): 연결 정보\n\\(\\hat A\\): 연결 정보에 자기 자신의 노드 추가\n\\(D\\): \\(A\\)를 표준화하기 위한 matrix\n\\(\\hat D^{-1/2} \\hat A \\hat D^{-1/2}\\): 연결정보에 대한 matrix\n\\(\\hat D^{-1/2} \\hat A \\hat D^{-1/2} X\\): \\(X\\)를 평행이동한 느낌\n\\(\\Theta\\): weight를 곱하는 과정\n-\n\\(X\\): \\(N\\)행의 행렬\n\\(y\\): 길이 \\(N\\)의 벡터, 사기거래 레이블\n\\(\\cal I\\): cc_num의 집합\n\\(T_i, i \\in \\cal I\\): 고객의 거래 시간\n\\({\\cal D} := \\{ (X_{i,t}, y_{i,t}): i \\in {\\cal I}, t \\in \\cal T_i \\}\\)\n\\({\\cal V} = \\{ v_{i,t}: i \\in \\cal I, t \\in \\cal T_i \\}\\)\n\\(|{\\cal V}| = \\sum_{i \\in \\cal I} |T_i| = \\cal N\\)\n\\({\\epsilon} = \\cup_{i \\in {\\cal I}} \\{ ( v_{i,t}, v_{i,s}) : t,s \\in T_{i}, t\\neq s\\}\\)\n\\(W_i : exp( \\frac{-{|t-s|}_2^2}{\\theta})\\)\n\\(W: N \\times N\\) matrix"
  },
  {
    "objectID": "posts/Att01.html",
    "href": "posts/Att01.html",
    "title": "[DL] Att01",
    "section": "",
    "text": "해당 강의노트는 전북대학교 김광수교수님 2023-2 고급딥러닝 자료임\n\n\n## https://www.tensorflow.org/tutorials/keras/regression?hl=ko ##\n\nimport pathlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)\n\n2.14.0\n\n\n\ndataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\ndataset_path\n\n'/root/.keras/datasets/auto-mpg.data'\n\n\n\ncolumn_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n                'Acceleration', 'Model Year', 'Origin']\nraw_dataset = pd.read_csv(dataset_path, names=column_names,\n                      na_values = \"?\", comment='\\t',\n                      sep=\" \", skipinitialspace=True)\n\ndataset = raw_dataset.copy()\ndataset.tail()\n\n\n  \n    \n\n\n\n\n\n\nMPG\nCylinders\nDisplacement\nHorsepower\nWeight\nAcceleration\nModel Year\nOrigin\n\n\n\n\n393\n27.0\n4\n140.0\n86.0\n2790.0\n15.6\n82\n1\n\n\n394\n44.0\n4\n97.0\n52.0\n2130.0\n24.6\n82\n2\n\n\n395\n32.0\n4\n135.0\n84.0\n2295.0\n11.6\n82\n1\n\n\n396\n28.0\n4\n120.0\n79.0\n2625.0\n18.6\n82\n1\n\n\n397\n31.0\n4\n119.0\n82.0\n2720.0\n19.4\n82\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndataset = dataset.dropna()\norigin = dataset.pop('Origin')\n#dataset['USA'] = (origin == 1)*1.0\ndataset['Europe'] = (origin == 2)*1.0\ndataset['Japan'] = (origin == 3)*1.0\ndataset.tail()\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dataset['USA'] = (origin == 1)*1.0\n&lt;ipython-input-5-f05403a9f198&gt;:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dataset['Europe'] = (origin == 2)*1.0\n&lt;ipython-input-5-f05403a9f198&gt;:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dataset['Japan'] = (origin == 3)*1.0\n\n\n\n  \n    \n\n\n\n\n\n\nMPG\nCylinders\nDisplacement\nHorsepower\nWeight\nAcceleration\nModel Year\nUSA\nEurope\nJapan\n\n\n\n\n393\n27.0\n4\n140.0\n86.0\n2790.0\n15.6\n82\n1.0\n0.0\n0.0\n\n\n394\n44.0\n4\n97.0\n52.0\n2130.0\n24.6\n82\n0.0\n1.0\n0.0\n\n\n395\n32.0\n4\n135.0\n84.0\n2295.0\n11.6\n82\n1.0\n0.0\n0.0\n\n\n396\n28.0\n4\n120.0\n79.0\n2625.0\n18.6\n82\n1.0\n0.0\n0.0\n\n\n397\n31.0\n4\n119.0\n82.0\n2720.0\n19.4\n82\n1.0\n0.0\n0.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntrain_dataset = dataset.sample(frac=0.8,random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\ntrain_stats = train_dataset.describe()\ntrain_stats.pop(\"MPG\")\ntrain_stats = train_stats.transpose()\ntrain_stats\n\n\ntrain_labels = train_dataset.pop('MPG')\ntest_labels = test_dataset.pop('MPG')\ndef norm(x):\n  return (x - train_stats['mean']) / train_stats['std']\nnormed_train_data = norm(train_dataset)\nnormed_test_data = norm(test_dataset)\n\n\ndef build_model():\n  model = keras.Sequential([\n\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),  # 'linear' instead of 'relu'\n    #layers.Dense(64, activation='relu'),\n    #layers.Dense(64, activation='relu'),\n    layers.Dense(1) ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n  return model\n\n\nmodel = build_model()\nmodel.summary()\n\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\n\nEPOCHS = 500\nhistory = model.fit(\n  normed_train_data, train_labels,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[PrintDot()])\n\nModel: \"sequential_15\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_53 (Dense)            (None, 64)                640       \n                                                                 \n dense_54 (Dense)            (None, 64)                4160      \n                                                                 \n dense_55 (Dense)            (None, 64)                4160      \n                                                                 \n dense_56 (Dense)            (None, 64)                4160      \n                                                                 \n dense_57 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 13185 (51.50 KB)\nTrainable params: 13185 (51.50 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n\n\n\ndef plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n\n  plt.figure(figsize=(8,12))\n\n  plt.subplot(2,1,1)\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error [MPG]')\n  plt.plot(hist['epoch'], hist['mae'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mae'],\n           label = 'Val Error')\n  plt.ylim([0,5])\n  plt.legend()\n\n  plt.subplot(2,1,2)\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error [$MPG^2$]')\n  plt.plot(hist['epoch'], hist['mse'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mse'],\n           label = 'Val Error')\n  plt.ylim([0,20])\n  plt.legend()\n  plt.show()\n\nplot_history(history)\n\n\n\n\n\ntest_predictions = model.predict(normed_test_data).flatten()\nyy = np.array(test_predictions)\nxx = np.array(normed_test_data[\"Weight\"])\nprint('MSE', np.mean(yy-test_labels)**2)\n\nidx = np.array(np.argsort(xx), dtype='int')\nprint(idx)\nxx = xx[idx]\nyy = yy[idx]\n\n3/3 [==============================] - 0s 4ms/step\nMSE 3.2270409256119708\n[ 9 65 44 56 38 24 76 57 74 66 43 45 40 37 67 60 10 52 41 21 13 30 68  3\n 50 75 34 61 11 64 73 25 27 49 72 77  4 62 23 63 59 22 33 31 18 47 36  8\n 51 46 48 32 29 69 58 70 28 39 71 16 26  0 55 53 15 54 42 12 14  6 35 20\n 17  5 19  1  2  7]\n\n\n\nplt.scatter(xx,yy)\nplt.show()"
  },
  {
    "objectID": "posts/[ADL] HW1.html",
    "href": "posts/[ADL] HW1.html",
    "title": "[ADL] Take Home",
    "section": "",
    "text": "해당 강의노트는 전북대학교 김광수교수님 2023-2 고급딥러닝 자료임\n제출기한~11/10"
  },
  {
    "objectID": "posts/[ADL] HW1.html#a",
    "href": "posts/[ADL] HW1.html#a",
    "title": "[ADL] Take Home",
    "section": "(a)",
    "text": "(a)\nCalculate \\(\\dfrac{∂ℓ}{∂w^{(l)}_{kj}}\\) for \\((l, k, j) ∈ \\{4, 1\\} × \\{1 \\} × \\{1, 2\\}\\)"
  },
  {
    "objectID": "posts/[ADL] HW1.html#b",
    "href": "posts/[ADL] HW1.html#b",
    "title": "[ADL] Take Home",
    "section": "(b)",
    "text": "(b)\nAssume that we have two mini-batches such that \\(\\{x = (2, 3), y = 6\\}\\) and \\(\\{x = (1, 4), y = 7\\}\\), and \\(ℓ = (y − f)^2\\) . Also, we initialize all values of \\(w^{(l)}_{kj} , w_j , b^{(l)}\\) and \\(b\\) as \\(1/10\\). Consider the updating rule of \\(w_{t+1} = w_t − ϵ∇_wℓ_t\\)."
  },
  {
    "objectID": "posts/[ADL] HW1.html#c",
    "href": "posts/[ADL] HW1.html#c",
    "title": "[ADL] Take Home",
    "section": "(c)",
    "text": "(c)\nCalculate the updated value of \\(w^{(2)}_{1,2}\\) at the first step only using the first batch."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boram-coco",
    "section": "",
    "text": "Everyday with Coco"
  }
]